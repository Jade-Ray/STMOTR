__base__: 
  desc: the base config yaml
  value: base.yaml
# * Train & Eval
model_name:
  desc: name of model
  value: mmotr

# * Transformer
num_encoder_layers:
  desc: Number of encoding layers in the transformer
  value: 6
num_decoder_layers:
  desc: Number of decoding layers in the transformer
  value: 6
dim_feedforward:
  desc: Intermediate size of the feedforward layers in the transformer blocks
  value: 2048
d_model:
  desc: Size of the embeddings (dimension of the transformer)
  value: 256
dropout:
  desc: Dropout applied in the transformer
  value: 0.1
nheads:
  desc: Number of attention heads inside the transformer's attentions
  value: 8
num_queries:
  desc: Number of query slots
  value: 50

# * Loss
aux_loss:
  desc: enable auxiliary decoding losses (loss at each layer)
  value: false

# * Matcher
set_cost_is_referred:
  desc: soft tokens coefficient in the matching cost
  value: 2
set_cost_bbox:
  desc: bbox giou l1 coefficient in the matching cost
  value: 5
set_cost_giou:
  desc: bboxes giou coefficient in the matching cost
  value: 1

# * Loss coefficients
is_referred_loss_coef:
  value: 2
boxes_loss_coef:
  value: 5
giou_loss_coef:
  value: 1
eos_coef:
  desc: Relative classification weight of the no-object class
  value: 0.3
alpha:
  desc: Weighting factor of focal loss.
  value: 0.75
gamma: 
  desc: Exponent of the modulating factor of focal loss.
  value: 2.0
