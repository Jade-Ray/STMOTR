__base__: 
  desc: the base config yaml
  value: base.yaml
# * Train & Eval
model_name:
  desc: name of model
  value: deformable_mmotr
enable_amp:
  desc: whether to enable automatic mixed precision during training
  value: false

# * Deformable Transformer
num_encoder_layers:
  desc: Number of encoding layers in the transformer
  value: 6
num_decoder_layers:
  desc: Number of decoding layers in the transformer
  value: 6
dim_feedforward:
  desc: Intermediate size of the feedforward layers in the transformer blocks
  value: 1024
d_model:
  desc: Size of the embeddings (dimension of the transformer)
  value: 256
dropout:
  desc: Dropout applied in the transformer
  value: 0.1
nheads:
  desc: Number of attention heads inside the transformer's attentions
  value: 8
start_level:
  desc: The start level of backbone features to embed
  value: 1
extra_levels:
  desc: Extra levels need to embed conved from last backbone feature
  value: 1
npoints:
  desc: Number of attention points inside the deformable's attentions
  value: 4
num_queries:
  desc: Number of query slots
  value: 50

# * Loss
aux_loss:
  desc: enable auxiliary decoding losses (loss at each layer)
  value: false

# * Matcher
set_cost_is_referred:
  desc: soft tokens coefficient in the matching cost
  value: 2
set_cost_bbox:
  desc: bbox giou l1 coefficient in the matching cost
  value: 5
set_cost_giou:
  desc: bboxes giou coefficient in the matching cost
  value: 1

# * Loss coefficients
is_referred_loss_coef:
  value: 2
boxes_loss_coef:
  value: 5
giou_loss_coef:
  value: 1
alpha:
  desc: Weighting factor of focal loss.
  value: 0.75
gamma: 
  desc: Exponent of the modulating factor of focal loss.
  value: 2.0


